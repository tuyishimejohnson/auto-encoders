{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuyishimejohnson/auto-encoders/blob/main/Johnson_Tuyishime_Network_Anomaly_Detection_using_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IH4sy6O23Tv"
      },
      "source": [
        "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOU8MXnI23T6"
      },
      "source": [
        "# Network Anomaly Detection using Autoencoders\n",
        "\n",
        "- Ananth Sankar, Solutions Architect at NVIDIA.\n",
        "- Eric Harper, Solutions Architect, Global Telecoms at NVIDIA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPnrxvE123T9"
      },
      "source": [
        "Welcome to the second lab of this series!\n",
        "\n",
        "In the previous lab we used XGBoost, a powerful and efficient tree-based algorithm for classification of anomalies. We were able to almost perfectly identify the anomalous data in the KDD99 dataset and which type of anomaly occurred.  However, in the real-world, *labeled* data can be expensive and hard to come by. Especially with network security, zero-day attacks can be the most challenging and also the most important attacks to detect. Since, by definition, these attacks are happening for the first time, there will be no way to have labels from them.\n",
        "\n",
        "So how do we approach *this* problem?\n",
        "\n",
        "For starters, we could have security analysts investigate the network packets and label anomalous ones. But that solution doesn't scale and our models might have difficulty identifying attacks that haven't occurred before.\n",
        "\n",
        "Our solution *needs to use* \"unsupervised learning.\" Unsupervised learning is the class of machine learning and deep learning algorithms that enable us to draw inferences from our dataset without labels.\n",
        "\n",
        "\n",
        "\n",
        "In this lab we will use autoencoders (AEs) to detect anomalies in the KDD99 dataset. There are a lot of advantages to using autoencoders for detecting anomalies. One main advantage is the that AEs can learn non-linear relationships in the data.\n",
        "\n",
        "While we will not be using the labels in the KDD99 dataset explicitly for model training, we will be using them to evaluate how well our model is doing at detecting the anomalies.  We will also use the labels to see if the AE is embedding the anomalies in latent space according to the type of anomaly.\n",
        "\n",
        "Note that we will be using Keras as the deep learning framework for this lab. Keras is an open source neural network library written in Python and it is designed to enable fast experimentation with deep neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3QDWBfqS23T_"
      },
      "outputs": [],
      "source": [
        "# Import libraries that will be needed for the lab\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "import os, datetime\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.utils import plot_model\n",
        "%load_ext tensorboard\n",
        "\n",
        "import pickle\n",
        "\n",
        "import random\n",
        "random.seed(123)\n",
        "\n",
        "data_path = './data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i-W2N4Y23UD"
      },
      "source": [
        "## Section 1: Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNfoGErp23UE"
      },
      "source": [
        "Let's load the preprocessed train and test data that we created in Lab 1. We'll think through the same research question we did in Lab 1 again since it helps us to understand exactly *how* our model is learning.\n",
        "\n",
        "*How does the ratio of anomalies to normal data impact results and why?*\n",
        "\n",
        "\n",
        "Recall that when using XGBoost, the ratio didn't impact training meaningfully. Anomalies were simply *a class* of our dataset, not made special in any way by their rare nature. Using AutoEncoders, you'll see that that's no longer true. We'll explore the questions of *how rare is rare enough?* and *how does that impact our ability to identify multiple classes of anomalies?*.\n",
        "\n",
        "In the cell below, choose to either use 1% or 5% anomaly in your dataset by setting the <code>pct_anomalies</code> parameter to .01 or .05 respectively. If you are taking this in an in-person workshop, choose a partner and do both so you can compare and contrast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLSosf7g23UF"
      },
      "outputs": [],
      "source": [
        "pct_anomalies = .01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10yg-EwZ23UH"
      },
      "outputs": [],
      "source": [
        "!python preprocess_data.py --pct_anomalies $pct_anomalies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8AS6ge323UM"
      },
      "outputs": [],
      "source": [
        "filename = './preprocessed_data_full.pkl'\n",
        "input_file = open(filename,'rb')\n",
        "preprocessed_data = pickle.load(input_file)\n",
        "input_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeibtrZH23UQ"
      },
      "source": [
        "Recall that we have constructed train and test sets where we removed most of anomalous data in the KDD99 dataset.  This lets us simulate a more realistic anomaly detection problem where anomalies only comprise a small percentage of the data.  We also trained a label encoder on the anomalous labels.  This will allow us to go back and forth between labels and their encoded values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdYFBJFG23US"
      },
      "outputs": [],
      "source": [
        "for key in preprocessed_data:\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhwciTji23UU"
      },
      "outputs": [],
      "source": [
        "le = preprocessed_data['le']\n",
        "x_train = preprocessed_data['x_train']\n",
        "y_train = preprocessed_data['y_train']\n",
        "x_test = preprocessed_data['x_test']\n",
        "y_test = preprocessed_data['y_test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxgGPLpm23UV"
      },
      "source": [
        "### 1.1 Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t3FTROw23UW"
      },
      "source": [
        "Most of the data preprocessing has already been done in Lab 1. We one-hot encoded the categorical variables and separated the labels off from the input data. After this the data was ready for the XGBoost model.  For training deep autoencoder models, the input data will also have to be scaled between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqsnixj223UX"
      },
      "outputs": [],
      "source": [
        "# Normalize the testing and training data using the MinMaxScaler from the scikit learn package\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Make sure to only fit the scaler on the training data\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "# convert the data to FP32\n",
        "x_train = x_train.astype(np.float32)\n",
        "x_test = x_test.astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui5yTIag23UY"
      },
      "source": [
        "## Section 2: Deep Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXB9w6LQ23UY"
      },
      "source": [
        "Autoencoders are a subset of neural network architectures where the output dimension is the same as the input dimension. Autoencoders have two networks, an encoder and a decoder.  The encoder encodes its input data into a smaller dimensional space, called the latent space.  The decoder network tries to reconstruct the original data from the latent encoding. Typically, the encoder and decoder are symmetric, and the latent space is a bottleneck. The autoencoder has to learn essential characteristics of the data to be able to do a high-quality reconstruction of the data during decode.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "In this lab, we'll be using deep autoencoders with dropout. Dropout is a way to control overfitting by randomly\n",
        "omitting subsets of features at each iteration of a training procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NilmLUEL23UZ"
      },
      "source": [
        "### 2.1 Keras Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNgHXxoV23UZ"
      },
      "source": [
        "Next we will chose the hyperparameters for the Keras autoencoder model.\n",
        "\n",
        "- batch_size: this determines how many datapoints we use for each gradient update. Choosing a large batch size will make the model train faster but it might not result in the best accuracy or generalization.\n",
        "\n",
        "- latent_dim: this determines the size of our bottleneck. Higher values add network capacity while lower values increase the efficiency of the encoding.\n",
        "\n",
        "- max_epochs: should be high enough for the network to learn from the data, but not so high as to overfit the training data or diverge to a worse result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1Adh9_J23Ua"
      },
      "outputs": [],
      "source": [
        "input_dim = x_train.shape[1]\n",
        "\n",
        "# model hyperparameters\n",
        "batch_size = 512\n",
        "\n",
        "latent_dim = 4\n",
        "\n",
        "max_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eQjbW_T23Ub"
      },
      "source": [
        "### 2.2 Encoder Network\n",
        "\n",
        "Here we define the encoder network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_nrer-W23Ub"
      },
      "outputs": [],
      "source": [
        "# The encoder will consist of a number of dense layers that decrease in size\n",
        "# as we taper down towards the bottleneck of the network, the latent space\n",
        "input_data = Input(shape=(input_dim,), name='encoder_input')\n",
        "\n",
        "# hidden layers\n",
        "encoder = Dense(96,activation='tanh', name='encoder_1')(input_data)\n",
        "encoder = Dropout(.1)(encoder)\n",
        "encoder = Dense(64,activation='tanh', name='encoder_2')(encoder)\n",
        "encoder = Dropout(.1)(encoder)\n",
        "encoder = Dense(48,activation='tanh', name='encoder_3')(encoder)\n",
        "encoder = Dropout(.1)(encoder)\n",
        "encoder = Dense(16,activation='tanh', name='encoder_4')(encoder)\n",
        "encoder = Dropout(.1)(encoder)\n",
        "\n",
        "# bottleneck layer\n",
        "latent_encoding = Dense(latent_dim, activation='linear', name='latent_encoding')(encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBZdW6OB23Uc"
      },
      "source": [
        "Here we instantiate the encoder model, look at a summary of its layers, and then visualize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGCtdDGy23Ud"
      },
      "outputs": [],
      "source": [
        "encoder_model = Model(input_data, latent_encoding)\n",
        "\n",
        "encoder_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4VRNv_A23Ue"
      },
      "outputs": [],
      "source": [
        "plot_model(\n",
        "    encoder_model,\n",
        "    to_file='encoder_model.png',\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir='TB' # TB for top to bottom, LR for left to right\n",
        ")\n",
        "\n",
        "Image(filename='encoder_model.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NgmKZDq23Uf"
      },
      "source": [
        "### 2.3 Decoder Network [Exercise]\n",
        "\n",
        "The decoder network comes after the bottle neck of the auto encoder. It takes latent values as input and then outputs reconstructions of the original input data.\n",
        "\n",
        "**Exercise: Use the fact that the decoder is a mirror image of the encoder network to replace the ##FIXME##s below and the fact that the input of each layer should be the output of the previous layer to replace the ##Replace with... statements.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqUfTPoo23Uf"
      },
      "outputs": [],
      "source": [
        "# The decoder network is a mirror image of the encoder network\n",
        "decoder = Dense(##FIXME##, activation='tanh', name='decoder_1')(##Replace with the layer name that feeds this one##)\n",
        "decoder = Dropout(.1)(decoder)\n",
        "decoder = Dense(##FIXME##, activation='tanh', name='decoder_2')(##Replace with the layer name that feeds this one##)\n",
        "decoder = Dropout(.1)(decoder)\n",
        "decoder = Dense(##FIXME##, activation='tanh', name='decoder_3')(##Replace with the layer name that feeds this one##)\n",
        "decoder = Dropout(.1)(decoder)\n",
        "decoder = Dense(##FIXME##, activation='tanh', name='decoder_4')(##Replace with the layer name that feeds this one##)\n",
        "decoder = Dropout(.1)(decoder)\n",
        "\n",
        "# The output is the same dimension as the input data we are reconstructing\n",
        "reconstructed_data = Dense(input_dim, activation='linear', name='reconstructed_data')(##Replace with the layer name that feeds this one##)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNl8rYsM23Ug"
      },
      "source": [
        "We instantiate the autoencoder model, look at a summary of it's layers, and visualize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqQXTbIh23Uh"
      },
      "outputs": [],
      "source": [
        "autoencoder_model = Model(input_data, reconstructed_data)\n",
        "\n",
        "autoencoder_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8RdyIY-23Uh"
      },
      "outputs": [],
      "source": [
        "plot_model(\n",
        "    autoencoder_model,\n",
        "    to_file='autoencoder_model.png',\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir='TB' # TB for top to bottom, LR for left to right\n",
        ")\n",
        "\n",
        "Image(filename='autoencoder_model.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69a43YGn23Ui"
      },
      "source": [
        "### 2.4 Compile the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z87yfBVc23Ui"
      },
      "source": [
        "Here we specify the loss function [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) and the optimizer [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQA2KWTW23Ui"
      },
      "outputs": [],
      "source": [
        "opt = optimizers.Adam(learning_rate=.00001)\n",
        "\n",
        "autoencoder_model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzTNNXqJ23Uj"
      },
      "source": [
        "### 2.5 Train on the KDD99 Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vW0m75-23Uj"
      },
      "source": [
        "Let us look at the feature and label columns of our training set before we start training our XG Boost model. Notice we are not using any labels, `y_train` or `y_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN4gtDrd23Uj"
      },
      "outputs": [],
      "source": [
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,profile_batch=0,update_freq='epoch',histogram_freq=1)\n",
        "\n",
        "train_history = autoencoder_model.fit(x_train, x_train,\n",
        "        shuffle=True,\n",
        "        epochs=max_epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_test, x_test),\n",
        "        callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F07EPLza23Uk"
      },
      "source": [
        "Let's inspect the loss on the train and validation sets.  You should see the loss on the training data and the loss on the validation data converging towards zero.  Notice that the training loss is actually higher than the validation loss.  That's because when we train the network, we are using dropout, which again, \"is a way to control overfitting by randomly omitting subsets of features at each iteration of a training procedure.\" When we validate, we remove the dropout, which gives our network its full strength."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMPw7dPv23Uk"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_history.history['loss'])\n",
        "plt.plot(train_history.history['val_loss'])\n",
        "plt.legend(['loss on train data', 'loss on validation data'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd78jkJk23Uk"
      },
      "source": [
        "### 2.6 Inspect the TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZk-4AQv23Us"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdfTuoJW23Ut"
      },
      "source": [
        "## Section 3: Anomaly Detection\n",
        "\n",
        "In order to detect anomalies, we need to do the following:\n",
        "\n",
        "1. Reconstruct the test data.\n",
        "2. Compute the reconstruction scores using MSE.\n",
        "3. Set a threshold to label test datapoints as anomalies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtqdSXJh23Ut"
      },
      "outputs": [],
      "source": [
        "# Reconstruct the data using our trainined autoencoder model.\n",
        "x_test_recon = autoencoder_model.predict(x_test)\n",
        "\n",
        "# the reconstruction score is the mean of the reconstruction errors (relatively high scores are anomalous)\n",
        "reconstruction_scores = np.mean((x_test - x_test_recon)**2, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSbLRy7E23Ut"
      },
      "source": [
        "Let's analyze the reconstruction scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJPOXE0223Uu"
      },
      "outputs": [],
      "source": [
        "# store the reconstruction data in a Pandas dataframe\n",
        "anomaly_data = pd.DataFrame({'recon_score':reconstruction_scores})\n",
        "\n",
        "# if our reconstruction scores our normally distributed we can use their statistics\n",
        "anomaly_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qblXyufs23Uu"
      },
      "outputs": [],
      "source": [
        "# plotting the density will give us an idea of how the reconstruction scores are distributed\n",
        "plt.xlabel('Reconstruction Score')\n",
        "anomaly_data['recon_score'].plot.hist(bins=200, range=[-.01, .03])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCrspbUO23Uu"
      },
      "source": [
        "Now we define a function to convert the labels to binary. Recall that the label encoder from the previous lab encoded normal data as 11. `labels`=11 therefore corresponds to normal data and `labels`!=11 corresponds to anomalous data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkLUo_0Y23Uv"
      },
      "outputs": [],
      "source": [
        "def convert_label_to_binary(label_encoder, labels):\n",
        "    normal_idx = np.where(label_encoder.classes_ == 'normal.')[0][0]\n",
        "    my_labels = labels.copy()\n",
        "    my_labels[my_labels != normal_idx] = 1\n",
        "    my_labels[my_labels == normal_idx] = 0\n",
        "    return my_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTbqSaMQ23Uv"
      },
      "source": [
        "## Section 4: Model Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8oGMjHF23Uv"
      },
      "outputs": [],
      "source": [
        "# convert our labels to binary\n",
        "binary_labels = convert_label_to_binary(le, y_test)\n",
        "\n",
        "# add the binary labels to our anomaly dataframe\n",
        "anomaly_data['binary_labels'] = binary_labels\n",
        "\n",
        "# let's check if the reconstruction statistics are different for labeled anomalies\n",
        "anomaly_data.groupby(by='binary_labels').describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M29zcJT023Uw"
      },
      "source": [
        "We can see from the above that the anomalous data has a mean reconstruction score of </b>0.05</b> while the normal data has a score of <b>~0.004</b>. This is a good sign that our autoencoder has learned to reconstruct normal data but fails to reconstruct anomalous data.\n",
        "\n",
        "Let's see how our ROC curve looks with the reconstruction score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksqY2TpR23Uw"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(binary_labels, reconstruction_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(fpr, tpr, lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='lime', linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD7KMbua23Ux"
      },
      "source": [
        "The ROC Curve looks great and we have a perfect AUC score.  This is a good sign, but we still need to verify that we can set a threshold which accomplishes our business objective. In this lab we are looking to detect as many anomalies as possible while also minimizing the number of false positives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBfwuAJ-23Ux"
      },
      "source": [
        "### 4.1 Thresholds\n",
        "\n",
        "There are number of ways to set thresholds.  We give some examples below and we encourage you to experiment by setting your own thresholds and see how they affect the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9JepfJs23Ux"
      },
      "outputs": [],
      "source": [
        "# We can pick the threshold based on maximizing the true positive rate (tpr)\n",
        "# and minimizing the false positive rate (fpr)\n",
        "optimal_threshold_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold = thresholds[optimal_threshold_idx]\n",
        "print(optimal_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWbrSIxb23Uy"
      },
      "outputs": [],
      "source": [
        "# Or we assume our reconstructions are normally distributed and label anomalies as those\n",
        "# that are a number of standard deviations away from the mean\n",
        "recon_mean = np.mean(reconstruction_scores)\n",
        "recon_stddev = np.std(reconstruction_scores)\n",
        "\n",
        "stats_threshold = recon_mean + 5*recon_stddev\n",
        "print(stats_threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks2njTGh23Uy"
      },
      "source": [
        "Now, similar to lab1, let us plot the confusion matrix in order to visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7fH7vTj23Uz"
      },
      "outputs": [],
      "source": [
        "# play around here and find the threshold that works for you\n",
        "\n",
        "#thresh = optimal_threshold\n",
        "thresh = stats_threshold\n",
        "\n",
        "\n",
        "\n",
        "print(thresh)\n",
        "\n",
        "pred_labels = (reconstruction_scores > thresh).astype(int)\n",
        "\n",
        "results = confusion_matrix(binary_labels, pred_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isZvWoIk23Uz"
      },
      "source": [
        "### 4.2 Confusion Matrix\n",
        "\n",
        "Note, in the next lab, you will be asked to implement your own confusion matrix since it's a very useful tool we want to draw your attention to. Copy, paste, and store this cell somewhere safe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHcaGg7d23Uz"
      },
      "outputs": [],
      "source": [
        "print ('Confusion Matrix: ')\n",
        "\n",
        "def plot_confusion_matrix(cm, target_names, title='Confusion Matrix', cmap=plt.cm.Greens):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(target_names))\n",
        "    plt.xticks(tick_marks, target_names, rotation=45)\n",
        "    plt.yticks(tick_marks, target_names)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    width, height = cm.shape\n",
        "\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            plt.annotate(str(cm[x][y]), xy=(y, x),\n",
        "                        horizontalalignment='center',\n",
        "                        verticalalignment='center')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "\n",
        "\n",
        "plot_confusion_matrix(results, ['Normal','Anomaly'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ-coHVk23U0"
      },
      "source": [
        "Analyzing the confusion matrix, we see that we get optimal results.  The algorithm is labeling most of the normal data anomalous data correctly while mislabeling very infrequently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-p8HjHi23U0"
      },
      "source": [
        "Returning to our research question: How does the proportion of anomalies to normal data impact the ability for a Deep Learning based AutoEncoder model to detect them?"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "MDM7NYoc23U0"
      },
      "source": [
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrqQRQ5H23U0"
      },
      "source": [
        "## Section 5: Multi-class Classification\n",
        "\n",
        "This exercise will show how we could potentially create labeled data by using k-means clustering algorithm to group our predicted anomalies in latent space. The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity.\n",
        "\n",
        "For each test datapoint that we've labeled as an anomaly, we will label the clusters based on the label that appears most in the cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBdqg5tg23U1"
      },
      "outputs": [],
      "source": [
        "# subset the test data so that we are only looking at the predicted anomalies\n",
        "x_test_df = pd.DataFrame(x_test)\n",
        "\n",
        "test_anomalies = x_test_df[pred_labels.astype('bool')]\n",
        "\n",
        "y_test_df = pd.DataFrame(y_test)\n",
        "test_anomalies_labels = y_test_df[pred_labels.astype('bool')]\n",
        "\n",
        "# encode the test anomalies into latent space\n",
        "encoded_test_anomalies = encoder_model.predict(test_anomalies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "holUIEau23U1"
      },
      "outputs": [],
      "source": [
        "# apply KMeans to the data in order to create clusters of anomalies\n",
        "kmeans = KMeans(n_clusters=10, random_state=123)\n",
        "kmeans.fit(encoded_test_anomalies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8g244zq23U1"
      },
      "outputs": [],
      "source": [
        "clusters = pd.DataFrame({'cluster':kmeans.labels_, 'label':test_anomalies_labels[0]})\n",
        "\n",
        "most_frequent_labels = clusters.groupby('cluster').label.value_counts()\n",
        "\n",
        "print(most_frequent_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woewGWai23U2"
      },
      "source": [
        "If we replace our clusters with the label that shows up most frequently for each cluster, then we can compare how well the anomalies are clustered together in latent space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT6pzqje23U2"
      },
      "outputs": [],
      "source": [
        "# this dictionary will map our cluster values to the labels that appear most frequently\n",
        "cluster_to_label = {}\n",
        "for cluster in range(0, 10):\n",
        "    label = most_frequent_labels[cluster].index[0]\n",
        "    cluster_to_label[cluster] = label\n",
        "\n",
        "# we then replace the clusters inplace by their label values\n",
        "clusters.cluster.replace(cluster_to_label, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWX718Ah23U2"
      },
      "source": [
        "Now we can plot the confusion matrix to see how well our clusters agree with the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMYP-Xg_23U3"
      },
      "outputs": [],
      "source": [
        "target_names = np.unique(list(clusters.label))\n",
        "cm = confusion_matrix(clusters.label, clusters.cluster)\n",
        "\n",
        "print ('Confusion Matrix :')\n",
        "\n",
        "def plot_confusion_matrix(cm,target_names, title='Confusion matrix', cmap=plt.cm.Greens):\n",
        "    plt.figure(figsize=(10,10),)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(target_names))\n",
        "    plt.xticks(tick_marks, target_names, rotation=45)\n",
        "    plt.yticks(tick_marks, target_names)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    width, height = cm.shape\n",
        "\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            plt.annotate(str(cm[x][y]), xy=(y, x),\n",
        "                        horizontalalignment='center',\n",
        "                        verticalalignment='center')\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "plot_confusion_matrix(cm,target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCoRBEQ_23U3"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZy9MSya23U3"
      },
      "source": [
        "- We were successfully able to detect KDD99 anomalies without using the labels for training by using a deep autoencoder.\n",
        "- We then showed that autoencoder model was embedding the anomalous datapoints in latent space according to the type of anomaly.\n",
        "- This means we could begin labeling the anomalous data by having SMEs look at a small fraction of the total data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_vFzgMx23U4"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhcYwDgi23U4"
      },
      "source": [
        "1. Haowen, Chen, Zhao, Li, Zeyan, Zhihan, . . . Honglin. (2018, February 12). [_Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications_](https://arxiv.org/abs/1802.03903).\n",
        "1. Ellison, D. (n.d.). [_Fraud Detection Using Autoencoders in Keras with a TensorFlow Backend_](https://www.datascience.com/blog/fraud-detection-with-tensorflow).\n",
        "1. Sadanand Singh (n.d.). _A Practical guide to Autoencoders_. Previously retrieved from https://sadanand-singh.github.io/posts/autoencoders/ but no longer online as of November 2023.\n",
        "1. Jeremy Jordan (n.d.). [_Variational Autoencoders_](https://www.jeremyjordan.me/variational-autoencoders/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AtM-6aq23U4"
      },
      "source": [
        "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}